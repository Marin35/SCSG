{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Kossi Neroma\n",
    "# * Marin Bouthemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Non-Convex Finite-Sum Optimization Via SCSG Methods</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**The paper** : [Non-Convex Finite-Sum Optimization Via SCSG Methods](https://arxiv.org/abs/1706.09156)\n",
    "\n",
    ">Here, we will be applying [SCSG]((https://arxiv.org/abs/1706.09156)) to convex/non convex optimization problems, mainly a neural network loss function. <br> <br> \n",
    "The first example, even if modelised as a neural network is just a logistic regression (two layer perceptron) which lost function is clearly convex.<br> <br>\n",
    "The second one concerns a deeper neural network and here the cost function is no more convex. Can our algorithm find the  optimal solution and thus make us get the best ever accuracy ? Let's  see ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, numpy as np\n",
    "\n",
    "from models import SGD, SCSG # two objects that implements, respectively, the Stochastic Dradient Descent algorithm (SGD)\n",
    "                            # and the stochastically controlled stochastic gradient (SCSG)\n",
    "import importlib, models\n",
    "importlib.reload(models)\n",
    "from models import SGD, SCSG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Laod the data\n",
    "> Our toy dataset would be the **mnist** one. This is a classical database of *70 000 handwritten digits*. Let's recall that this dataset is natively avaialable in tensorflow, our main library for gradient computing (**automatic differenciation**) and neural network modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# def one_hot(x):\n",
    "#     v = np.zeros((len(x), x.max() +1))\n",
    "#     v[np.arange(len(x)), x] = 1\n",
    "#     return v\n",
    "\n",
    "\n",
    "# y_train, y_test = one_hot(y_train), one_hot(y_test)\n",
    "# x_train, x_test = x_train.reshape((-1, 28*28)), x_test.reshape((-1, 28*28))\n",
    "\n",
    "# x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.31529 acuracy:  0.9604\n",
      "Epoch: 2 cost = 0.10463 acuracy:  0.9629\n",
      "Epoch: 3 cost = 0.07484 acuracy:  0.96416664\n",
      "Epoch: 4 cost = 0.05787 acuracy:  0.9669\n",
      "Epoch: 5 cost = 0.04567 acuracy:  0.96666\n",
      "Epoch: 6 cost = 0.03752 acuracy:  0.9676167\n",
      "Epoch: 7 cost = 0.02949 acuracy:  0.9675429\n",
      "Epoch: 8 cost = 0.02593 acuracy:  0.96895\n",
      "Epoch: 9 cost = 0.02545 acuracy:  0.97003335\n",
      "Epoch: 10 cost = 0.02046 acuracy:  0.97071\n"
     ]
    }
   ],
   "source": [
    "model = SGD(0.5, 50, 10)\n",
    "model.archi = [28*28, 256, 128, 10]\n",
    "\n",
    "model.fit(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost: 0.24301 acuracy:  0.1028\n",
      "Epoch: 2 cost: 0.36431 acuracy:  0.1004\n",
      "Epoch: 3 cost: 0.09005 acuracy:  0.1207\n",
      "Epoch: 4 cost: 0.05360 acuracy:  0.143275\n",
      "Epoch: 5 cost: 0.04405 acuracy:  0.13422\n",
      "Epoch: 6 cost: 0.03613 acuracy:  0.14115\n",
      "Epoch: 7 cost: 0.03053 acuracy:  0.13802858\n",
      "Epoch: 8 cost: 0.02518 acuracy:  0.1489125\n",
      "Epoch: 9 cost: 0.02324 acuracy:  0.17042223\n",
      "Epoch: 10 cost: 0.01870 acuracy:  0.18746\n"
     ]
    }
   ],
   "source": [
    "model = SCSG(0.5, 50, 10)\n",
    "model.archi = [28*28, 256, 128, 10]\n",
    "\n",
    "model.fit(x_train, y_train, x_test, y_test,eta = 0.5,  B = 50, b = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
